## 爬虫概述

获取网页并提取和保存信息的自动化程序

### 获取网页

获取网页的源代码，向服务器发送一个请求，获得的响应体便是网页源代码。有很多lib可以帮我们实现HTTP请求和响应操作：urllib, requests等。

### 提取信息

获取网页源代码后，接下来就是分析网页源代码，从中提取数据。

- 正则表达式：最通用，但是复杂而且容易出错
- 根据网页节点属性、CSS选择器或Xpath来提取：Beautiful Soup、pyquery、lxml 

### 保存数据

保存形式很多，可以简单保存为txt或者json文本，也可以保存到数据库，如MySQL, MongoDB等，还可以保存到远程服务器，如借助SFTP进行操作

## 能抓怎样的数据

最常抓取的就是HTML源代码

但是有些网页可能不返回HTML代码，而是一个JSON字符串（API结构大多是这样），这种数据同样可以抓取，并且便于数据提取。

还可以抓取二进制数据：图片、视频、音频等

还可以看到各种拓展名的文件，如CSS, JS和配置文件等

上述内容都对应各自的URL，基于HTTP/HTTPS协议的，只要是这样的数据，爬虫都可以爬取。

## JS渲染页面

有时候使用urllib, requests抓取网页时，得到的源代码和浏览器中看到的不一样。

这可能是由于整个网页可能都是由JS渲染出来的，原始的HTML可能就是一个空壳，例如：

```html
	<!DOCTYPE html>
<html>
<head>
<meta charset="UTF-8">
<title>This is a Demo</title>
</head>
<body>
<div id="container">
</div>
</body>
<script src="app.js"></script>
</html>

```

body节点里只有一个id="container"的节点，后面的script标签里面的app.js才是渲染整个网页的。在浏览器中打开这个页面时，首先会加载这个 HTML 内容，接着浏览器会发现其中引入了一个 app.js 文件，然后便会接着去请求这个文件，获取到该文件后，便会执行其中的 JavaScript 代码，而 JavaScript 则会改变 HTML 中的节点，向其添加内容，最后得到完整的页面。

但是在用 urllib 或 requests 等库请求当前页面时，我们得到的只是这个 HTML 代码，它不会帮助我们去继续加载这个 JavaScript 文件，这样也就看不到浏览器中的内容了。

这也解释了为什么有时我们得到的源代码和浏览器中看到的不一样。

因此，使用基本 HTTP 请求库得到的源代码可能跟浏览器中的页面源代码不太一样。对于这样的情况，我们可以分析其后台 Ajax 接口，也可使用 Selenium、Splash 这样的库来实现模拟 JavaScript 渲染。

